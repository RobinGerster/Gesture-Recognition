{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nataxcan\\miniconda3\\envs\\compviz2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataloader import EmbeddingsDataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "from model import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "from train import run_training, Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample run\n",
    "# r = Run()\n",
    "# losses, test_accs, best_model = r.run_experiment()\n",
    "# r.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m model, width \u001b[39m=\u001b[39m elems\n\u001b[0;32m     28\u001b[0m r \u001b[39m=\u001b[39m Run(model\u001b[39m=\u001b[39mmodel, frames_per_datapoint\u001b[39m=\u001b[39mwidth, epochs\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, train_data_overlap\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 29\u001b[0m losses, test_accs, best_model \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39;49mrun_experiment()\n\u001b[0;32m     30\u001b[0m torch\u001b[39m.\u001b[39msave(best_model, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbest_model_exp\u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m_combination\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_width\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(width)\u001b[39m}\u001b[39;00m\u001b[39m.save\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39m# sum the results (for averaging later)\u001b[39;00m\n",
      "File \u001b[1;32md:\\GHDesktop\\Gesture-Recognition\\train.py:100\u001b[0m, in \u001b[0;36mRun.run_experiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_experiment\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 100\u001b[0m     losses, test_accs, best_model \u001b[39m=\u001b[39m run_training(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_rate, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size,\\\n\u001b[0;32m    101\u001b[0m                  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframes_per_datapoint, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_data_overlap, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs)\n\u001b[0;32m    102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_accs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model \u001b[39m=\u001b[39m losses, test_accs, best_model\n\u001b[0;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m losses, test_accs, best_model\n",
      "File \u001b[1;32md:\\GHDesktop\\Gesture-Recognition\\train.py:76\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m(model, learning_rate, batch_size, frames_per_datapoint, train_data_overlap, epochs)\u001b[0m\n\u001b[0;32m     74\u001b[0m correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m corr\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(train_dataloader) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m20\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m     test_acc \u001b[39m=\u001b[39m test()\n\u001b[0;32m     77\u001b[0m     test_accs\u001b[39m.\u001b[39mappend(test_acc)\n\u001b[0;32m     78\u001b[0m     test_acc_last \u001b[39m=\u001b[39m test_acc\n",
      "File \u001b[1;32md:\\GHDesktop\\Gesture-Recognition\\train.py:33\u001b[0m, in \u001b[0;36mrun_training.<locals>.test\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m test_dataloader:\n\u001b[0;32m     32\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 33\u001b[0m     res \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     34\u001b[0m     \u001b[39m# label = one_hot(y[:,-1].type(torch.int64), num_classes=9).float()\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     total_attempts \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nataxcan\\miniconda3\\envs\\compviz2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\GHDesktop\\Gesture-Recognition\\model.py:97\u001b[0m, in \u001b[0;36mLstmRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 97\u001b[0m     x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[0;32m     98\u001b[0m     x \u001b[39m=\u001b[39m x[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\n\u001b[0;32m     99\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mc:\\Users\\nataxcan\\miniconda3\\envs\\compviz2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\nataxcan\\miniconda3\\envs\\compviz2\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# this is a simple experiment to see if there is a link\n",
    "# between model capacity and performance\n",
    "models = [\n",
    "    LstmRNN,\n",
    "    LstmRNN2,\n",
    "    LstmRNN3\n",
    "]\n",
    "widths = [\n",
    "    4,\n",
    "    16,\n",
    "    32,\n",
    "    64\n",
    "]\n",
    "combinations = list(itertools.product(models, widths))\n",
    "number_of_experiments_to_average_over = 3\n",
    "raw_results = []\n",
    "results = []\n",
    "\n",
    "for i, elems in enumerate(combinations):\n",
    "    raw_results.append([])\n",
    "    losses_sum = None\n",
    "    test_accs_sum = None\n",
    "    for j in range(number_of_experiments_to_average_over):\n",
    "        # run the experiment\n",
    "        model, width = elems\n",
    "        r = Run(model=model, frames_per_datapoint=width, epochs=4, train_data_overlap=True)\n",
    "        losses, test_accs, best_model = r.run_experiment()\n",
    "        torch.save(best_model, f'best_model_exp{j}_combination{i}_width{str(width)}.save')\n",
    "        # sum the results (for averaging later)\n",
    "        raw_results.append((max(test_accs), losses, test_accs, str((model, width))))\n",
    "        if isinstance(losses_sum, type(None)):\n",
    "            losses_sum = np.array(losses)\n",
    "            test_accs_sum = np.array(test_accs)\n",
    "        else:\n",
    "            losses_sum += np.array(losses)\n",
    "            test_accs_sum += np.array(test_accs)\n",
    "    # average the results\n",
    "    avg_losses = losses_sum / number_of_experiments_to_average_over\n",
    "    avg_test_accs = test_accs_sum / number_of_experiments_to_average_over\n",
    "    results.append((max(avg_test_accs), str(elems), str(model), width, avg_losses, avg_test_accs))\n",
    "    print(i, elems, 'done')\n",
    "# which combination gave the best average performance?\n",
    "results.sort()\n",
    "[print(R[:4]) for R in results]\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import colorsys\n",
    "from matplotlib import colors\n",
    "\n",
    "def get_shades(color, n):\n",
    "    color = colors.to_rgba(color)\n",
    "    hsv_color = np.array(colorsys.rgb_to_hsv(*color[:3]))\n",
    "    step = hsv_color[2] / (n + 1)\n",
    "    shades = [(*colorsys.hsv_to_rgb(*(hsv_color * (1, 1, i * step))), color[3]) for i in range(1, n+1)]\n",
    "    return shades\n",
    "\n",
    "n = 5\n",
    "print(\"Colour shades:\", get_shades('red', n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(r[2:4], round(r[0], 2)) for r in results]\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nlayer_perceptron_comparison_data.pickle', 'wb+') as f:\n",
    "    import pickle\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "for color, model in zip(model_colors, models):\n",
    "    model_name = str(model)\n",
    "    model_name_stripped = model_name.split(\"'\")[1].split('.')[1]\n",
    "    results_model = [elem for elem in results if model_name in elem[1]]\n",
    "    results_model.sort(key=lambda x: x[3]) # sort by increasing number of frames\n",
    "    shades = get_shades(color, len(results_model))\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for acc, nframes in [(x[5], x[3]) for x in results_model]:\n",
    "        plt.plot(range(len(acc)), acc, label=f\"{model_name_stripped} with {nframes} frames\", color=shades.pop())\n",
    "    plt.title(f'for model {model_name_stripped}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "# for maxacc, idstr, model, contextlength, avg_losses, avg_test_accs in results_mlp1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_scores = []\n",
    "width_stds = []\n",
    "for width in widths:\n",
    "    scores = np.array([x[0] for x in results if x[3] == width])\n",
    "    width_scores.append(np.average(scores))\n",
    "    width_stds.append(scores.std())\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.errorbar(widths, width_scores, width_stds)\n",
    "plt.xticks(widths)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# (max(avg_test_accs), str(elems), str(model), width, avg_losses, avg_test_accs)\n",
    "#         0                1            2        3        4            5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([type(r[6]) for r in results][6])\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
