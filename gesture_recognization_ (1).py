# -*- coding: utf-8 -*-
"""Gesture recognization .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D-OIyxlclo4TrGsAQN3OPawObxMtSFU8
"""



import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam

import lightning as L
from torch.utils.data import TensorDataset, DataLoader

class LSTM(L.LightningModule):
#Create and initialize Weight and Bias
  def __init__(self):
    super().__init__()
    mean = torch.tensor(0.0)
    std = torch.tensor(1.0)

 # All parameters here should be trained
    self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std, size=(1,384)), requires_grad=True)
    self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std, size=(1,384)), requires_grad=True)
    self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)

    self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std, size=(1,384)), requires_grad=True)
    self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std, size=(1,384)), requires_grad=True)
    self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)

    self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std, size=(1,384)), requires_grad=True)
    self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std, size=(1,384)), requires_grad=True)
    self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)

    self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std, size=(1,384)), requires_grad=True)
    self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std, size=(1,384)), requires_grad=True)
    self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)

 # Do the LSTM math 
  def lstm_unit(self, input_value, long_memory, short_memory):
    # Calculate the percentage of the long-term memory to remenber.
    long_remember_percnet = torch.sigmoid((short_memory * self.wlr1) + 
                        (input_value * self.wlr2) + 
                        self.blr1)
    
    # Create a new potential long term memory and determine what percentage of it to remember.
    potential_remember_percent = torch.sigmoid((short_memory * self.wpr1) +
                          (input_value * self.wpr2) + 
                           self.bpr1)
      
    potential_memory = torch.tanh((short_memory * self.wp1) +
                    (input_value * self.wp2) +
                    self.bp1)
    
    # Update the long term memory.
    updated_long_memory = ((long_memory * long_remember_percnet) + 
                (potential_remember_percent * potential_memory))
    
    # Create a new short term memory and determine what percentage to remember.
    output_percent = torch.sigmoid((short_memory * self.wo1) +
                    (input_value * self.wo2) +
                     self.bo1)
    
    updated_short_memory = torch.tanh(updated_long_memory) * output_percent

    # Return the long and short term memories
    return([updated_long_memory, updated_short_memory])

# Make a forward pass through unrolled LSTM
  def forward(self, input):
    long_memory = 0
    short_memory = 0
    frame1 = input[0,:]
    frame2 = input[1,:]
    frame3 = input[2,:]
    frame4 = input[3,:]

    long_memory, short_memory = self.lstm_unit(frame1, long_memory, short_memory)
    long_memory, short_memory = self.lstm_unit(frame2, long_memory, short_memory)
    long_memory, short_memory = self.lstm_unit(frame3, long_memory, short_memory)
    long_memory, short_memory = self.lstm_unit(frame4, long_memory, short_memory)

    return(short_memory)

  def configure_optimizers(self):
    return Adam(self.parameters())
# Calculate loss and log trainning progress                          )
  def training_step(self, batch, batch_idx):
    input_i, label_i = batch
    output_i = self.forward(input_i[0])
    loss = (output_i - label_i) ** 2

    self.log("train_loss", loss)
    return loss

model = LSTM()
inputs = torch.rand(4,384)
labels = torch.tensor([0., 1., 2., 3.])

dataset = TensorDataset(inputs, labels)
dataloader = DataLoader(dataset)

trainer = L.Trainer(max_epochs=100000)
trainer.fit(model, train_dataloaders=dataloader)



